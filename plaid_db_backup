#!/usr/bin/python
import os
import sys
import socket
from datetime import datetime
from sh import sudo, pg_dumpall, bzip2, mkdir, chmod
from hashlib import md5
import boto
from boto.s3.key import Key

BACKUP_DIRECTORY = "/var/plaid/backups"
BACKUP_BUCKET = "plaid_database_backups"
with open('/root/.aws/access_key', 'r') as f:
  AWS_ACCESS_KEY = f.read().strip()
with open('/root/.aws/secret_key', 'r') as f:
  AWS_SECRET_KEY = f.read().strip()

def get_file_name(prefix):
  "Create a filename from prefix plus weekday plus hour."
  d = datetime.now()
  w = ['mon', 'tue', 'wed', 'thur', 'fri', 'sat', 'sun'][d.weekday()]
  return "%s-%s.sql.bz2" % (prefix, w)

def ensure_backup_directory(d):
  "If directory doesn't exist, create it."
  if not os.path.isdir(d):
    mkdir(d, '-p')

def create_database_dump(f):
  "Dump all databases using pg_dumpall to file 'f'."
  o = f.replace('.bz2', '')
  sudo.su('postgres', '-c', 'pg_dumpall', _out=o)
  bzip2('-f', o)

def hash_of_file(f):
  "Calculate the md5sum of file 'f'."
  md5_hash = md5()
  with open(f, 'rb') as f:
    while True:
      d = f.read(8192)
      md5_hash.update(d)
      if len(d) < 8192:
        break
  return md5_hash.hexdigest()

def stored_hash_of_file(f):
  "Retrieve the stored md5sum has of file 'f'."
  try:
    with open(BACKUP_DIRECTORY + '/.s3.current.hash', 'r') as fh:
      return fh.read()
  except:
    return

def store_hash_of_file(f, h):
  "Save the md5sum of a file in another file."
  with open(BACKUP_DIRECTORY+'/.s3.current.hash', 'w') as fh:
    fh.write(h)

def store_file_in_bucket(f, b):
  "Put file 'f' in S3 bucket 'b'."
  s3 = boto.connect_s3(AWS_ACCESS_KEY, AWS_SECRET_KEY)
  bucket = s3.lookup(b) or s3.create_bucket(b)
  key = Key(bucket)
  key.key = '%s.latest.sql.bz2' % socket.gethostname()
  with open(f, 'r') as f:
    key.set_contents_from_file(f)

def copy_to_s3(f, h):
  "Copy a file to S3 but only if the stored hash is different."
  if h == stored_hash_of_file(f):
    print "Files match; doing nothing."
    return
  else:
    print "Copying to S3."
    store_file_in_bucket(f, BACKUP_BUCKET)
    store_hash_of_file(f, h)


if __name__ == "__main__":
  # determine current file's name
  backup_file = get_file_name("pg_dumpall")

  # make sure the backup directory is present
  ensure_backup_directory(BACKUP_DIRECTORY)

  # create a pg_dumpall named for the weekday and hour
  create_database_dump(BACKUP_DIRECTORY + '/' + backup_file)

  # store a hash of it in the backups directory
  backup_file_hash = hash_of_file(
      BACKUP_DIRECTORY + '/' + backup_file)

  # if the hash changes, copy it to S3
  copy_to_s3(
      BACKUP_DIRECTORY + '/' + backup_file, backup_file_hash)
